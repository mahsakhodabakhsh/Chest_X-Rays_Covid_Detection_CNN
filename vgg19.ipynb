{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134b0cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== Memory Information ========================================\n",
      "Total: 8.00GB\n",
      "Available: 4.41GB\n",
      "Used: 2.98GB\n",
      "Percentage: 44.8%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "def get_size(bytes, suffix=\"B\"):\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "print(\"=\"*40, \"Memory Information\", \"=\"*40)\n",
    "svmem = psutil.virtual_memory()\n",
    "print(f\"Total: {get_size(svmem.total)}\") ; print(f\"Available: {get_size(svmem.available)}\")\n",
    "print(f\"Used: {get_size(svmem.used)}\") ; print(f\"Percentage: {svmem.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6ab88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import AveragePooling2D,MaxPool2D, Dropout, Flatten, Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cbfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of our data on drive\n",
    "data_dir =  r'/gdrive/My Drive/WorkShopDataset'\n",
    " \n",
    "# Loading Data\n",
    "data = load_files(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a290d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=os.listdir(\"/gdrive/My Drive/WorkShopDataset\")\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the data and labels to Numpy arrays\n",
    "X = np.array(data['filenames'])\n",
    "y = np.array(data['target'])\n",
    "labels = np.array(data['target_names'])\n",
    " \n",
    "# How the arrays look like?\n",
    "print('Data files - ',X[0])\n",
    "print('Target labels - ',y[0])\n",
    "print('Number of training files : ', X.shape[0])\n",
    "print('Number of training targets : ', y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_to_arr(file_path_list):\n",
    "    arr = []\n",
    "    img_width, img_height = 224,224 \n",
    "    #Loop over the image paths\n",
    "    for file_path in file_path_list:\n",
    "        \"\"\"\n",
    "           Load the image, swap color channels, and resize it to be a fixed\n",
    "           224*224 pixels while ignoring aspect ratio\n",
    "        \"\"\"\n",
    "        img = load_img(file_path, target_size = (img_width, img_height))\n",
    "        img = img_to_array(img)\n",
    "        \n",
    "        #update the data\n",
    "        arr.append(img)\n",
    "    return arr\n",
    "# Here our data is updated and it's stocked in the X array again !\n",
    "X = np.array(convert_img_to_arr(X))\n",
    "\n",
    "# The Data Shape\n",
    "print(X.shape) \n",
    "print('First training item : ',X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at first 5 training data. \n",
    "fig = plt.figure(figsize = (16,9))\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(1,5,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow((X[i].astype(np.uint8)))\n",
    "    plt.title(folders[y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c03d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   After that data is converted into Numpy array, Now, \n",
    "   Let's scale the pixel intenties to the range[0,255]\n",
    "\"\"\"\n",
    "X = X.astype('float32')/255\n",
    "\n",
    "# Let's confirm the number of classes :) \n",
    "no_of_classes = len(np.unique(y))\n",
    "no_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ade787",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea96c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "let's converts a class vector (integers) to binary class matrix by performing the \n",
    "one-hot encoding on the labels\n",
    "\"\"\"\n",
    "y = np.array(np_utils.to_categorical(y,no_of_classes))\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec980649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's splite the data into subsets and explore their shapes !\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "print('The train Data Shape ', X_train.shape[0])\n",
    " \n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test,y_test, test_size = 0.5)\n",
    "print('The validation Data Shape ', X_valid.shape[0])\n",
    "print('The test Data Shape ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The train Data Shape ', X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_model = keras.applications.VGG19(input_shape= X_train.shape[1:], include_top=False, weights=\"imagenet\")\n",
    "# setting the VGG model to be untrainable.\n",
    "VGG19_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75452856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "Model = Sequential()\n",
    "Model.add(VGG19_model)\n",
    "Model.add(BatchNormalization())\n",
    "Model.add(Dropout(0.20))\n",
    "Model.add(Flatten())\n",
    "Model.add(Dense(256,activation='relu'))\n",
    "Model.add(Dense(2,activation='softmax'))\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "bs = 8\n",
    "optimizer = Adam(lr=1e-4)\n",
    "Model.compile(optimizer, loss = 'binary_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f722e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to train our model !\n",
    "epochs = 50\n",
    "\n",
    "#initialize the training data augmentation object\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    zoom_range = 0.1, \n",
    "    width_shift_range=0.1, \n",
    "    height_shift_range=0.1,  \n",
    "    horizontal_flip=True)\n",
    " \n",
    "checkpointer = ModelCheckpoint(filepath = \"/gdrive/My Drive/NouredAminaVGG19m.h5\", save_best_only = True, verbose=1)\n",
    "start = time.time()\n",
    " \n",
    "# let's get started !\n",
    " \n",
    "history=Model.fit_generator(train_datagen.flow(X_train, y_train, batch_size = bs),\n",
    "                            steps_per_epoch = len(X_train)//bs,\n",
    "                            validation_data = (X_valid, y_valid),\n",
    "                            validation_steps = len(X_valid)//bs,\n",
    "                            epochs =epochs,\n",
    "                            callbacks= [checkpointer])\n",
    " \n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print ('\\n This Model took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration/60, epochs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = Model.evaluate(  \n",
    "     X_test, y_test, batch_size=bs, verbose=2)\n",
    " \n",
    "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print(\"Loss: {}\".format(eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2493eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the train/validation loss and accuracy wrt epochs\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "def plot(history):\n",
    "    plt.figure(1) \n",
    "     # summarize history for accuracy  \n",
    " \n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history.history['accuracy'])  \n",
    "    plt.plot(history.history['val_accuracy'])  \n",
    "    plt.title('accuracy vs val_accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')  \n",
    " \n",
    "     # summarize history for loss  \n",
    " \n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history.history['loss'])  \n",
    "    plt.plot(history.history['val_loss'])  \n",
    "    plt.title('loss vs val_loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    " \n",
    "# Finaly, let's call the plot function with the 'result' parameter \n",
    " \n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcf47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize some random test prediction.\n",
    "def visualize_pred(y_pred):\n",
    "# plot a random sample of test images, their predicted labels, and ground truth\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    for i, idx in enumerate(np.random.choice(X_test.shape[0], size=16, replace=False)):\n",
    "        ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(X_test[idx]))\n",
    "        pred_idx = np.argmax(y_pred[idx])\n",
    "        true_idx = np.argmax(y_test[idx])\n",
    "        ax.set_title(\"{} ({})\".format(labels[pred_idx], labels[true_idx]),\n",
    "                     color=(\"green\" if pred_idx == true_idx else \"red\"))\n",
    "\n",
    "visualize_pred(Model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e656c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = Model.predict(X_test)\n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
    "Y_true = np.argmax(y_test,axis = 1)\n",
    "confusion_mtx = confusion_matrix(Y_true,Y_pred_classes)\n",
    "f,ax = plt.subplots(figsize = (8,8))\n",
    "sns.heatmap(confusion_mtx,annot=True,linewidths = 0.01,cmap=\"Greens\",\n",
    "            linecolor = \"gray\",fmt = \".2f\",ax=ax\n",
    "            )\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.argmax(axis=1),Y_pred_classes,  target_names= labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef340af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=keras.models.load_model(\"/gdrive/My Drive/NouredAminaVGG19m.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3953e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize some random test prediction.\n",
    "def visualize_pred(y_pred):\n",
    "# plot a random sample of test images, their predicted labels, and ground truth\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    for i, idx in enumerate(np.random.choice(X_test.shape[0], size=16, replace=False)):\n",
    "        ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(X_test[idx]))\n",
    "        pred_idx = np.argmax(y_pred[idx])\n",
    "        true_idx = np.argmax(y_test[idx])\n",
    "        ax.set_title(\"{} ({})\".format(labels[pred_idx], labels[true_idx]),\n",
    "                     color=(\"green\" if pred_idx == true_idx else \"red\"))\n",
    "\n",
    "visualize_pred(my_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish ----- > created by : Mahsa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
